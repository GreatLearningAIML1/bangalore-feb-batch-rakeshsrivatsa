{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84Q8JfvaeZZ6"
      },
      "source": [
        "## Linear Classifier in TensorFlow \n",
        "Using Low Level API in Eager Execution mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7f2494a-8e98-4902-9f9a-fca8d76bb9f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KiObW4V4SIOz",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4yQKMiJOB6R",
        "colab": {}
      },
      "source": [
        "pricing = pd.read_csv('/content/drive/My Drive/AIML_NoteBooks/LabAndAssignments/R6/prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K8pWsNQOB6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fb270353-bfe7-4438-bae4-e994f6822acf"
      },
      "source": [
        "pricing.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 851264 entries, 0 to 851263\n",
            "Data columns (total 7 columns):\n",
            "date      851264 non-null object\n",
            "symbol    851264 non-null object\n",
            "open      851264 non-null float64\n",
            "close     851264 non-null float64\n",
            "low       851264 non-null float64\n",
            "high      851264 non-null float64\n",
            "volume    851264 non-null float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 45.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "pricing = pricing.drop(['date','symbol'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
        "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_hG9rGBOB6s",
        "colab": {}
      },
      "source": [
        "pricing['volume'] = pricing['volume'].apply(lambda x: x/1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl2CQx-H2sxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pricing = pricing.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LE4U8lTdQJq",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X =  pricing.drop(\"volume\", axis=1)\n",
        "y =  pricing.pop(\"volume\")\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYK-aUuLbrz2"
      },
      "source": [
        "#### Convert Training and Test Data to numpy float32 arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ao-S0tQGcncz",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im1ZegbDdKgv"
      },
      "source": [
        "### Normalize the data\n",
        "You can use Normalizer from sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EkKAy7fOB6y",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "X_normalized = preprocessing.normalize(X, norm='l2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the Model in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L205qPeQOB7B",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
        "x_n = tf.nn.l2_normalize(x,1) \n",
        "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n",
        "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
        "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "2.Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JveGlx25OB7H",
        "colab": {}
      },
      "source": [
        "y = tf.add(tf.matmul(x_n,W),b,name='output') #X is normalized, hence X_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "3.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "loss_func = tf.reduce_mean(tf.square(y-y_),name='Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "4.Function to train the Model\n",
        "\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss_func)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Train the model for 100 epochs \n",
        "1. Observe the training loss at every iteration\n",
        "2. Observe Train loss at every 5th iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "training_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ca2126f-72f8-475d-ea12-c837b3ca05dc"
      },
      "source": [
        "print(\"Shape of W : \",W.shape)\n",
        "print(\"Shape of b : \",b.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of W :  (4, 1)\n",
            "Shape of b :  (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERq9GOKKciho"
      },
      "source": [
        "### Model Prediction on 1st Examples in Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKGvUWahcihp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f6cbb58d-b625-4a97-9cef-e159035c7a13"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss_func],feed_dict={x:X_train, y_:y_train}) # X and Y is like X_Train and Y_Train here.\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at step:  0  is  225.87958\n",
            "Training loss at step:  5  is  203.89143\n",
            "Training loss at step:  10  is  197.76749\n",
            "Training loss at step:  15  is  196.0619\n",
            "Training loss at step:  20  is  195.58687\n",
            "Training loss at step:  25  is  195.45459\n",
            "Training loss at step:  30  is  195.41774\n",
            "Training loss at step:  35  is  195.4075\n",
            "Training loss at step:  40  is  195.40462\n",
            "Training loss at step:  45  is  195.40382\n",
            "Training loss at step:  50  is  195.4036\n",
            "Training loss at step:  55  is  195.40355\n",
            "Training loss at step:  60  is  195.40353\n",
            "Training loss at step:  65  is  195.40355\n",
            "Training loss at step:  70  is  195.40353\n",
            "Training loss at step:  75  is  195.40353\n",
            "Training loss at step:  80  is  195.40353\n",
            "Training loss at step:  85  is  195.40353\n",
            "Training loss at step:  90  is  195.40355\n",
            "Training loss at step:  95  is  195.40353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxvzxZM4Jyvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92755c6e-0ad7-4ef3-e7f0-bde08863717e"
      },
      "source": [
        "test_loss = sess.run([loss_func],feed_dict={x:X_test, y_:y_test}) \n",
        "print ('Training loss at step:is ', test_loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at step:is  [241.01949]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "## Classification using tf.Keras\n",
        "\n",
        "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O0g6lorycihf"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6xFvb5sRcihg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "20e651cd-2ff4-48d8-fa28-6616d93bf978"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/AIML_NoteBooks/LabAndAssignments/R6/11_Iris.csv')\n",
        "data.info()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            "Id               150 non-null int64\n",
            "SepalLengthCm    150 non-null float64\n",
            "SepalWidthCm     150 non-null float64\n",
            "PetalLengthCm    150 non-null float64\n",
            "PetalWidthCm     150 non-null float64\n",
            "Species          150 non-null object\n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SAB--Qdwcihm"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IJr5dYnocihm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "83923c6a-f33e-4a5b-c8f4-36d718502b4d"
      },
      "source": [
        "data_new = pd.get_dummies(data,prefix=['Species'],columns=['Species'])\n",
        "data_new.info()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 8 columns):\n",
            "Id                         150 non-null int64\n",
            "SepalLengthCm              150 non-null float64\n",
            "SepalWidthCm               150 non-null float64\n",
            "PetalLengthCm              150 non-null float64\n",
            "PetalWidthCm               150 non-null float64\n",
            "Species_Iris-setosa        150 non-null uint8\n",
            "Species_Iris-versicolor    150 non-null uint8\n",
            "Species_Iris-virginica     150 non-null uint8\n",
            "dtypes: float64(4), int64(1), uint8(3)\n",
            "memory usage: 6.4 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D95nY5ILcihj"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RyMQoLMucihj",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X =  data_new.drop(['Id','Species_Iris-setosa','Species_Iris-versicolor','Species_Iris-virginica'], axis=1)\n",
        "y =  data_new[['Species_Iris-setosa','Species_Iris-versicolor','Species_Iris-virginica']]\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b22qpC5xcihr"
      },
      "source": [
        "###  Building Model in tf.keras\n",
        "\n",
        "Build a Linear Classifier model  <br>\n",
        "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
        "2. Apply Softmax on Dense Layer outputs <br>\n",
        "3. Use SGD as Optimizer\n",
        "4. Use categorical_crossentropy as loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hov_UFnUciht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3e6e400-74c8-4c4d-d76b-0a580083128a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5FdzqIKcihw"
      },
      "source": [
        "### Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4qLEdHPscihx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c8397bf6-a88b-43d6-bb1d-1e4b4a53707a"
      },
      "source": [
        "    model = Sequential()\n",
        "    model.add(Dense(3, input_dim=4, kernel_initializer='normal', activation='softmax'))#by default, kernel_initializer will be uniform distribution.\n",
        "    #model.add(Dense(3, kernel_initializer='normal'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics = ['accuracy']) #try this with adam or sgv"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 11:39:28.544868 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 11:39:28.548286 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 11:39:28.552772 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0721 11:39:28.569170 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 11:39:28.587576 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-SgSSdRcih5"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBgKZkhkcih6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "f76f75e6-f8f3-454c-caba-b64aa71394ce"
      },
      "source": [
        "model.fit(X_train, y_train,validation_data = (X_test, y_test), epochs=10, batch_size=5)\n",
        "scores = model.evaluate(X,y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 11:39:28.689574 139736798652288 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0721 11:39:28.737172 139736798652288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0376 - acc: 0.4857 - val_loss: 1.0026 - val_acc: 0.6000\n",
            "Epoch 2/10\n",
            "105/105 [==============================] - 0s 625us/step - loss: 0.9186 - acc: 0.6952 - val_loss: 0.8815 - val_acc: 0.7111\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 0s 522us/step - loss: 0.8605 - acc: 0.6952 - val_loss: 0.8406 - val_acc: 0.6000\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 0s 559us/step - loss: 0.7706 - acc: 0.7238 - val_loss: 0.7612 - val_acc: 0.6000\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 0s 501us/step - loss: 0.7381 - acc: 0.7619 - val_loss: 0.7604 - val_acc: 0.6000\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 0s 523us/step - loss: 0.6869 - acc: 0.7333 - val_loss: 0.6754 - val_acc: 0.7556\n",
            "Epoch 7/10\n",
            "105/105 [==============================] - 0s 585us/step - loss: 0.6448 - acc: 0.7905 - val_loss: 0.7404 - val_acc: 0.6000\n",
            "Epoch 8/10\n",
            "105/105 [==============================] - 0s 513us/step - loss: 0.6270 - acc: 0.7333 - val_loss: 0.6192 - val_acc: 0.9778\n",
            "Epoch 9/10\n",
            "105/105 [==============================] - 0s 496us/step - loss: 0.6217 - acc: 0.7429 - val_loss: 0.6317 - val_acc: 0.6000\n",
            "Epoch 10/10\n",
            "105/105 [==============================] - 0s 524us/step - loss: 0.5835 - acc: 0.7238 - val_loss: 0.6082 - val_acc: 0.6222\n",
            "150/150 [==============================] - 0s 66us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P32ASP1Vjt0a"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8rd0jjAjyTR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f66edcf-e145-4d42-ea1a-abb692494dad"
      },
      "source": [
        "scores[1]*100"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.33333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faE0-wLKU80k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('output.dump')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XiipRpe7rbVh"
      },
      "source": [
        "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
        "\n",
        "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5Du3lubr4sA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}